{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "double_u_net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abcdjdj/cs-766-project/blob/main/double_u_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWSJpsyKqHjH",
        "outputId": "0f848f7a-93de-4f16-c86a-8b149ff48ad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW78vWjm3eGq",
        "outputId": "15b6c92d-f1d3-4ec6-c08b-56f77061ee7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility Functions"
      ],
      "metadata": {
        "id": "tNWTHOQ_14id"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YngUNSbzeXl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm as tqdm\n",
        "\n",
        "'''\n",
        "Reads the image specified by 'path' and returns it\n",
        "param : path - path of image file\n",
        "return : image as a numpy array\n",
        "'''\n",
        "def read_img(path):\n",
        "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    image = np.clip(image - np.median(image)+127, 0, 255)\n",
        "    image = image/255.0\n",
        "    image = image.astype(np.float32)\n",
        "    return image\n",
        "\n",
        "def read_mask(path):\n",
        "    mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    mask = mask/255.0\n",
        "    mask = mask.astype(np.float32)\n",
        "    #mask = np.expand_dims(mask, axis=-1)\n",
        "    return mask\n",
        "\n",
        "'''\n",
        "Converts numpy img to tensor\n",
        "param : img - numpy arr containing image data\n",
        "return : t - torch tensor of shape [1, 3, H, W]\n",
        "'''\n",
        "def img_to_tensor(img):\n",
        "    t = torch.from_numpy(img)\n",
        "    t = t.view(-1, 3, t.shape[0], t.shape[1])\n",
        "    return t\n",
        "\n",
        "def mask_to_tensor(mask):\n",
        "    t = torch.from_numpy(mask)\n",
        "    t = t.view(-1, t.shape[0], t.shape[1])\n",
        "    return t\n",
        "\n",
        "'''\n",
        "Converts tensor back to numpy img\n",
        "param : t - torch tensor of shape [1, 3, H, W]\n",
        "return : img - numpy arr containing image data\n",
        "'''\n",
        "def tensor_to_img(t):\n",
        "    t = t.view(t.shape[2], t.shape[3], 3)\n",
        "    return t.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double U-Net Architecture"
      ],
      "metadata": {
        "id": "Z58WzYZp2O2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrap Up inside nn.Module"
      ],
      "metadata": {
        "id": "mZPm0tGIhYtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torchvision import models\n",
        "from imageio import imread as imread\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class SqueezeAndExcite(nn.Module):\n",
        "  def __init__(self, x, ratio = 8):\n",
        "    super(SqueezeAndExcite, self).__init__()\n",
        "\n",
        "    channel_axis = 1\n",
        "    filters = x.shape[channel_axis]\n",
        "    # Architecture\n",
        "    self.avgpool2d = nn.AvgPool2d(kernel_size = (x.shape[2], x.shape[3]))\n",
        "    self.sequential = nn.Sequential(nn.Linear(filters, filters//ratio, bias = False), nn.ReLU(), nn.Linear(filters//ratio, filters, bias = False), nn.Sigmoid())\n",
        "\n",
        "  def forward(self, x):\n",
        "    init = x\n",
        "    channel_axis = 1\n",
        "    filters = init.shape[channel_axis]\n",
        "    x = self.avgpool2d(x)\n",
        "    x = x.view(init.shape[0] , filters)\n",
        "    x = self.sequential(x)\n",
        "    x = x.view(init.shape[0], filters, 1, 1)\n",
        "\n",
        "    return torch.mul(init, x)\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, x, filters):\n",
        "      super(ConvBlock, self).__init__()\n",
        "\n",
        "      self.layer1_conv2d = nn.Conv2d(in_channels = x.shape[1], out_channels = filters, kernel_size = 3, padding='same')\n",
        "      x = self.layer1_conv2d(x)\n",
        "      self.layer1_batchnorm2d = nn.BatchNorm2d(num_features = x.shape[1])\n",
        "      x = self.layer1_batchnorm2d(x)\n",
        "      self.layer1_relu = nn.ReLU()\n",
        "      x = self.layer1_relu(x)\n",
        "\n",
        "      self.layer2_conv2d = nn.Conv2d(in_channels = x.shape[1], out_channels = filters, kernel_size = 3, padding='same')\n",
        "      x = self.layer2_conv2d(x)\n",
        "      self.layer2_batchnorm2d = nn.BatchNorm2d(num_features = x.shape[1])\n",
        "      x = self.layer2_batchnorm2d(x)\n",
        "      self.layer2_relu = nn.ReLU()\n",
        "      x = self.layer2_relu(x)\n",
        "\n",
        "      self.squeeze_and_excite = SqueezeAndExcite(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.layer1_conv2d(x)\n",
        "      x = self.layer1_batchnorm2d(x)\n",
        "      x = self.layer1_relu(x)\n",
        "\n",
        "      x = self.layer2_conv2d(x)\n",
        "      x = self.layer2_batchnorm2d(x)\n",
        "      x = self.layer2_relu(x)\n",
        "\n",
        "      x = self.squeeze_and_excite.forward(x)\n",
        "      return x\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, x, filter_count):\n",
        "      super(ASPP, self).__init__()\n",
        "\n",
        "      self.layer1_avgpool2d = nn.AvgPool2d(kernel_size = (x.shape[2], x.shape[3]))\n",
        "      se = self.layer1_avgpool2d(x)\n",
        "      self.layer1_conv2d = nn.Conv2d(in_channels = se.shape[1], out_channels = filter_count, kernel_size = 1, padding='same')\n",
        "      se = self.layer1_conv2d(se)\n",
        "      self.layer1_batchnorm2d = nn.BatchNorm2d(num_features = se.shape[1])\n",
        "      se = self.layer1_batchnorm2d(se)\n",
        "      self.layer1_relu = nn.ReLU()\n",
        "      se = self.layer1_relu(se)\n",
        "      self.layer1_upsampling = nn.UpsamplingBilinear2d(size=(x.shape[2], x.shape[3]))\n",
        "      se = self.layer1_upsampling(se)\n",
        "\n",
        "      self.layer2_conv2d = nn.Conv2d(dilation=1, in_channels = x.shape[1], out_channels = filter_count, kernel_size = 1, padding='same', bias=False)\n",
        "      y1 = self.layer2_conv2d(x)\n",
        "      self.layer2_batchnorm2d = nn.BatchNorm2d(num_features = y1.shape[1])\n",
        "      y1 = self.layer2_batchnorm2d(y1)\n",
        "      self.layer2_relu = nn.ReLU()\n",
        "      y1 = self.layer2_relu(y1)\n",
        "\n",
        "      self.layer3_conv2d = nn.Conv2d(dilation=6, in_channels = x.shape[1], out_channels = filter_count, kernel_size = 1, padding='same', bias=False)\n",
        "      y2 = self.layer3_conv2d(x)\n",
        "      self.layer3_batchnorm2d = nn.BatchNorm2d(num_features = y2.shape[1])\n",
        "      y2 = self.layer3_batchnorm2d(y2)\n",
        "      self.layer3_relu = nn.ReLU()\n",
        "      y2 = self.layer3_relu(y2)\n",
        "\n",
        "      self.layer4_conv2d = nn.Conv2d(dilation=12, in_channels = x.shape[1], out_channels = filter_count, kernel_size = 1, padding='same', bias=False)\n",
        "      y3 = self.layer4_conv2d(x)\n",
        "      self.layer4_batchnorm2d = nn.BatchNorm2d(num_features = y3.shape[1])\n",
        "      y3 = self.layer4_batchnorm2d(y3)\n",
        "      self.layer4_relu = nn.ReLU()\n",
        "      y3 = self.layer4_relu(y3)\n",
        "\n",
        "      self.layer5_conv2d = nn.Conv2d(dilation=18, in_channels = x.shape[1], out_channels = filter_count, kernel_size = 1, padding='same', bias=False)\n",
        "      y4 = self.layer5_conv2d(x)\n",
        "      self.layer5_batchnorm2d = nn.BatchNorm2d(num_features = y4.shape[1])\n",
        "      y4 = self.layer5_batchnorm2d(y4)\n",
        "      self.layer5_relu = nn.ReLU()\n",
        "      y4 = self.layer5_relu(y4)\n",
        "\n",
        "      y = torch.cat([se, y1, y2, y3, y4], dim=1)\n",
        "      self.layer6_conv2d = nn.Conv2d(dilation=1, in_channels = y.shape[1], out_channels = filter_count, kernel_size = 1, padding='same', bias=False)\n",
        "      y = self.layer6_conv2d(y)\n",
        "      self.layer6_batchnorm2d = nn.BatchNorm2d(num_features = y.shape[1])\n",
        "      y = self.layer6_batchnorm2d(y)\n",
        "      self.layer6_relu = nn.ReLU()\n",
        "      y = self.layer6_relu(y)\n",
        "\n",
        "    def forward(self, x, filter_count):\n",
        "      se = self.layer1_avgpool2d(x)\n",
        "      se = self.layer1_conv2d(se)\n",
        "      se = self.layer1_batchnorm2d(se)\n",
        "      se = self.layer1_relu(se)\n",
        "      se = self.layer1_upsampling(se)\n",
        "      #print(se.shape)\n",
        "\n",
        "      y1 = self.layer2_conv2d(x)\n",
        "      y1 = self.layer2_batchnorm2d(y1)\n",
        "      y1 = self.layer2_relu(y1)\n",
        "      #print(y1.shape)\n",
        "\n",
        "      y2 = self.layer3_conv2d(x)\n",
        "      y2 = self.layer3_batchnorm2d(y2)\n",
        "      y2 = self.layer3_relu(y2)\n",
        "      #print(y2.shape)\n",
        "\n",
        "      y3 = self.layer4_conv2d(x)\n",
        "      y3 = self.layer4_batchnorm2d(y3)\n",
        "      y3 = self.layer4_relu(y3)\n",
        "      #print(y3.shape)\n",
        "\n",
        "      y4 = self.layer5_conv2d(x)\n",
        "      y4 = self.layer5_batchnorm2d(y4)\n",
        "      y4 = self.layer5_relu(y4)\n",
        "      #print(y4.shape)\n",
        "\n",
        "      y = torch.cat([se, y1, y2, y3, y4], dim=1)\n",
        "      y = self.layer6_conv2d(y)\n",
        "      y = self.layer6_batchnorm2d(y)\n",
        "      y = self.layer6_relu(y)\n",
        "      #print(y.shape)\n",
        "      return y\n",
        "\n",
        "class Encoder1(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Encoder1, self).__init__()\n",
        "      self.model = models.vgg19()\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "      #skip connections from pre-trained VGG-19\n",
        "      names = [\"ReLU-4\", \"ReLU-9\", \"ReLU-18\", \"ReLU-27\", \"ReLU-36\"]\n",
        "\n",
        "      indices = [3, 8, 17, 26, 35]\n",
        "\n",
        "      skip_connections = []\n",
        "\n",
        "      def encoder1_receive_outputs(layer, _, output):\n",
        "          skip_connections.append(output)\n",
        "\n",
        "      for name, layer in self.model.named_children():\n",
        "          for idx in indices:\n",
        "              layer[idx].register_forward_hook(encoder1_receive_outputs)\n",
        "          break\n",
        "\n",
        "      self.model(inputs)\n",
        "\n",
        "      return skip_connections[-1], skip_connections[0:-1]\n",
        "\n",
        "class Decoder1(nn.Module):\n",
        "    def __init__(self, inputs, skip_connections):\n",
        "        super(Decoder1, self).__init__()\n",
        "        num_filters = [256, 128, 64, 32]\n",
        "        skip_connections.reverse()\n",
        "        x = inputs\n",
        "\n",
        "        self.layer1_upsampling = nn.UpsamplingBilinear2d(size=(2*x.shape[2], 2*x.shape[3]))\n",
        "        x = self.layer1_upsampling(x)\n",
        "        x = torch.cat([x, skip_connections[0]], dim=1)\n",
        "        self.layer1_convblock = ConvBlock(x, num_filters[0])\n",
        "        x = self.layer1_convblock.forward(x)\n",
        "\n",
        "        self.layer2_upsampling = nn.UpsamplingBilinear2d(size=(2*x.shape[2], 2*x.shape[3]))\n",
        "        x = self.layer2_upsampling(x)\n",
        "        x = torch.cat([x, skip_connections[1]], dim=1)\n",
        "        self.layer2_convblock = ConvBlock(x, num_filters[1])\n",
        "        x = self.layer2_convblock.forward(x)\n",
        "\n",
        "        self.layer3_upsampling = nn.UpsamplingBilinear2d(size=(2*x.shape[2], 2*x.shape[3]))\n",
        "        x = self.layer3_upsampling(x)\n",
        "        x = torch.cat([x, skip_connections[2]], dim=1)\n",
        "        self.layer3_convblock = ConvBlock(x, num_filters[2])\n",
        "        x = self.layer3_convblock.forward(x)\n",
        "\n",
        "        self.layer4_upsampling = nn.UpsamplingBilinear2d(size=(2*x.shape[2], 2*x.shape[3]))\n",
        "        x = self.layer4_upsampling(x)\n",
        "        x = torch.cat([x, skip_connections[3]], dim=1)\n",
        "        self.layer4_convblock = ConvBlock(x, num_filters[3])\n",
        "        x = self.layer4_convblock.forward(x)\n",
        "\n",
        "        # Undo the reversal so that forward passes don't get screwed\n",
        "        skip_connections.reverse()\n",
        "    \n",
        "    def forward(self, inputs, skip_connections):\n",
        "        num_filters = [256, 128, 64, 32]\n",
        "        skip_connections.reverse()\n",
        "        x = inputs\n",
        "\n",
        "        x = self.layer1_upsampling(x)\n",
        "        x = torch.cat([x, skip_connections[0]], dim=1)\n",
        "        x = self.layer1_convblock.forward(x)\n",
        "\n",
        "        x = self.layer2_upsampling(x)\n",
        "        x = torch.cat([x, skip_connections[1]], dim=1)\n",
        "        x = self.layer2_convblock.forward(x)\n",
        "\n",
        "        x = self.layer3_upsampling(x)\n",
        "        x = torch.cat([x, skip_connections[2]], dim=1)\n",
        "        x = self.layer3_convblock.forward(x)\n",
        "\n",
        "        x = self.layer4_upsampling(x)\n",
        "        x = torch.cat([x, skip_connections[3]], dim=1)\n",
        "        x = self.layer4_convblock.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Encoder2(nn.Module):\n",
        "    def __init__(self, inputs):\n",
        "        super(Encoder2, self).__init__()\n",
        "        num_filters = [32, 64, 128, 256]\n",
        "        x = inputs\n",
        "\n",
        "        self.layer1_convblock = ConvBlock(x, num_filters[0])\n",
        "        x = self.layer1_convblock.forward(x)\n",
        "        self.layer1_maxpool2d = nn.MaxPool2d(kernel_size = (2,2))\n",
        "        x = self.layer1_maxpool2d(x)\n",
        "\n",
        "        self.layer2_convblock = ConvBlock(x, num_filters[1])\n",
        "        x = self.layer2_convblock.forward(x)\n",
        "        self.layer2_maxpool2d = nn.MaxPool2d(kernel_size = (2,2))\n",
        "        x = self.layer2_maxpool2d(x)\n",
        "\n",
        "        self.layer3_convblock = ConvBlock(x, num_filters[2])\n",
        "        x = self.layer3_convblock.forward(x)\n",
        "        self.layer3_maxpool2d = nn.MaxPool2d(kernel_size = (2,2))\n",
        "        x = self.layer3_maxpool2d(x)\n",
        "\n",
        "        self.layer4_convblock = ConvBlock(x, num_filters[3])\n",
        "        x = self.layer4_convblock.forward(x)\n",
        "        self.layer4_maxpool2d = nn.MaxPool2d(kernel_size = (2,2))\n",
        "        x = self.layer4_maxpool2d(x)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        num_filters = [32, 64, 128, 256]\n",
        "        skip_connections = []\n",
        "        x = inputs\n",
        "\n",
        "        x = self.layer1_convblock.forward(x)\n",
        "        skip_connections.append(x)\n",
        "        x = self.layer1_maxpool2d(x)\n",
        "\n",
        "        x = self.layer2_convblock.forward(x)\n",
        "        skip_connections.append(x)\n",
        "        x = self.layer2_maxpool2d(x)\n",
        "\n",
        "        x = self.layer3_convblock.forward(x)\n",
        "        skip_connections.append(x)\n",
        "        x = self.layer3_maxpool2d(x)\n",
        "\n",
        "        x = self.layer4_convblock.forward(x)\n",
        "        skip_connections.append(x)\n",
        "        x = self.layer4_maxpool2d(x)\n",
        "\n",
        "        return x, skip_connections\n",
        "\n",
        "class Decoder2(nn.Module):\n",
        "      def __init__(self, inputs, skip_1, skip_2):\n",
        "          super(Decoder2, self).__init__()\n",
        "          num_filters = [256, 128, 64, 32]\n",
        "\n",
        "          skip_2.reverse()\n",
        "          x = inputs\n",
        "\n",
        "          self.layer1_upsampling = nn.UpsamplingBilinear2d(size=(2*x.shape[2], 2*x.shape[3]))\n",
        "          x = self.layer1_upsampling(x)\n",
        "          x = torch.cat([x, skip_1[0], skip_2[0]], dim=1)\n",
        "          self.layer1_convblock = ConvBlock(x, num_filters[0])\n",
        "          x = self.layer1_convblock.forward(x)\n",
        "\n",
        "          self.layer2_upsampling = nn.UpsamplingBilinear2d(size=(2*x.shape[2], 2*x.shape[3]))\n",
        "          x = self.layer2_upsampling(x)\n",
        "          x = torch.cat([x, skip_1[1], skip_2[1]], dim=1)\n",
        "          self.layer2_convblock = ConvBlock(x, num_filters[1])\n",
        "          x = self.layer2_convblock.forward(x)\n",
        "\n",
        "          self.layer3_upsampling = nn.UpsamplingBilinear2d(size=(2*x.shape[2], 2*x.shape[3]))\n",
        "          x = self.layer3_upsampling(x)\n",
        "          x = torch.cat([x, skip_1[2], skip_2[2]], dim=1)\n",
        "          self.layer3_convblock = ConvBlock(x, num_filters[2])\n",
        "          x = self.layer3_convblock.forward(x)\n",
        "\n",
        "          self.layer4_upsampling = nn.UpsamplingBilinear2d(size=(2*x.shape[2], 2*x.shape[3]))\n",
        "          x = self.layer4_upsampling(x)\n",
        "          x = torch.cat([x, skip_1[3], skip_2[3]], dim=1)\n",
        "          self.layer4_convblock = ConvBlock(x, num_filters[3])\n",
        "          x = self.layer4_convblock.forward(x)\n",
        "\n",
        "          skip_2.reverse() # Undo the reverse so we don't screw up forward()\n",
        "      \n",
        "      def forward(self, inputs, skip_1, skip_2):\n",
        "          num_filters = [256, 128, 64, 32]\n",
        "\n",
        "          skip_2.reverse()\n",
        "          x = inputs\n",
        "\n",
        "          x = self.layer1_upsampling(x)\n",
        "          x = torch.cat([x, skip_1[0], skip_2[0]], dim=1)\n",
        "          x = self.layer1_convblock.forward(x)\n",
        "\n",
        "          x = self.layer2_upsampling(x)\n",
        "          x = torch.cat([x, skip_1[1], skip_2[1]], dim=1)\n",
        "          x = self.layer2_convblock.forward(x)\n",
        "\n",
        "          x = self.layer3_upsampling(x)\n",
        "          x = torch.cat([x, skip_1[2], skip_2[2]], dim=1)\n",
        "          x = self.layer3_convblock.forward(x)\n",
        "\n",
        "          x = self.layer4_upsampling(x)\n",
        "          x = torch.cat([x, skip_1[3], skip_2[3]], dim=1)\n",
        "          x = self.layer4_convblock.forward(x)\n",
        "\n",
        "          return x\n",
        "\n",
        "class OutputBlock(nn.Module):\n",
        "      def __init__(self, inputs):\n",
        "          super(OutputBlock, self).__init__()\n",
        "          self.conv2d = nn.Conv2d(in_channels = inputs.shape[1], out_channels = 1, kernel_size = 1, padding = \"same\")\n",
        "          self.sigmoid = nn.Sigmoid()\n",
        "      \n",
        "      def forward(self, inputs):\n",
        "          x = self.conv2d(inputs)\n",
        "          x = self.sigmoid(x)\n",
        "          return x\n",
        "\n",
        "class DoubleUNet(nn.Module):\n",
        "  def __init__(self, inputs):\n",
        "      super(DoubleUNet, self).__init__()\n",
        "\n",
        "      # Encoder 1\n",
        "      self.encoder1 = Encoder1()\n",
        "      encoder1_op, encoder1_skip_conns = self.encoder1.forward(inputs)\n",
        "\n",
        "      # ASPP\n",
        "      self.aspp1 = ASPP(encoder1_op, 64)\n",
        "      aspp_op = self.aspp1.forward(encoder1_op, 64)\n",
        "\n",
        "      # Decoder 1\n",
        "      self.decoder1 = Decoder1(aspp_op, encoder1_skip_conns)\n",
        "      decoder1_op = self.decoder1.forward(aspp_op, encoder1_skip_conns)\n",
        "\n",
        "      # Output 1\n",
        "      self.outputblock1 = OutputBlock(decoder1_op)\n",
        "      mask = self.outputblock1.forward(decoder1_op)\n",
        "      network1_op = inputs * mask\n",
        "\n",
        "      # Encoder 2\n",
        "      self.encoder2 = Encoder2(network1_op)\n",
        "      encoder2_op,encoder2_skip_conns = self.encoder2.forward(network1_op)\n",
        "\n",
        "      # ASPP 2\n",
        "      self.aspp2 = ASPP(encoder2_op, 64)\n",
        "      aspp2_op = self.aspp2.forward(encoder2_op, 64)\n",
        "\n",
        "      # Decoder 2\n",
        "      self.decoder2 = Decoder2(aspp2_op, encoder1_skip_conns, encoder2_skip_conns)\n",
        "      decoder2_op = self.decoder2.forward(aspp2_op, encoder1_skip_conns, encoder2_skip_conns)\n",
        "\n",
        "      # Output 2\n",
        "      self.outputblock2 = OutputBlock(decoder2_op)\n",
        "      network2_op = self.outputblock2.forward(decoder2_op)\n",
        "\n",
        "      final_output = torch.cat([mask, network2_op], dim = 1)\n",
        "\n",
        "      #self.encoder1_vgg19 = models.vgg19()\n",
        "      #self.conv_block = ConvBlock(torch.ones(1, 3, 256, 256), filters = 8)\n",
        "      #self.squeeze = SqueezeAndExcite(torch.ones(1, 10, 256, 256))\n",
        "      # To get picked up - type(self.xxx) == nn.Module\n",
        "      #self.ASPP_model = ASPP(torch.ones(2,512,16,16), 64)\n",
        "      #self.encoder1 = Encoder1()\n",
        "      #self.decoder1 = Decoder1(torch.ones(1, 512, 16, 16), [torch.ones(1, 64, 256, 256), torch.ones(1, 128, 128, 128), torch.ones(1, 256, 64, 64), torch.ones(1, 512, 32, 32)])\n",
        "      #self.encoder2 = Encoder2(torch.ones(1, 3, 256, 256)) - NOT TESTED\n",
        "      #self.decoder2 = Decoder2(..) - NOT TESTED\n",
        "      #self.output = OutputBlock(torch.ones(1, 256, 64, 64))\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "      # Encoder 1\n",
        "      encoder1_op, encoder1_skip_conns = self.encoder1.forward(inputs)\n",
        "\n",
        "      # ASPP\n",
        "      aspp_op = self.aspp1.forward(encoder1_op, 64)\n",
        "\n",
        "      # Decoder 1\n",
        "      decoder1_op = self.decoder1.forward(aspp_op, encoder1_skip_conns)\n",
        "\n",
        "      # Output 1\n",
        "      mask = self.outputblock1.forward(decoder1_op)\n",
        "      network1_op = inputs * mask\n",
        "\n",
        "      # Encoder 2\n",
        "      encoder2_op,encoder2_skip_conns = self.encoder2.forward(network1_op)\n",
        "\n",
        "      # ASPP 2\n",
        "      aspp2_op = self.aspp2.forward(encoder2_op, 64)\n",
        "\n",
        "      # Decoder 2\n",
        "      decoder2_op = self.decoder2.forward(aspp2_op, encoder1_skip_conns, encoder2_skip_conns)\n",
        "\n",
        "      # Output 2\n",
        "      network2_op = self.outputblock2.forward(decoder2_op)\n",
        "\n",
        "      final_output = torch.cat([mask, network2_op], dim = 1)\n",
        "      return final_output"
      ],
      "metadata": {
        "id": "YfWoiD0OhZEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-Processing"
      ],
      "metadata": {
        "id": "E6Q0MUPzgytG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_list = sorted(glob.glob(\"out/image/*\"))\n",
        "mask_list = sorted(glob.glob(\"out/mask/*\"))"
      ],
      "metadata": {
        "id": "fghNfMsSDak0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#img_list = [img_to_tensor(read_img(ele)) for ele in img_list]\n",
        "#mask_list = [img_to_tensor(read_img(ele)) for ele in mask_list]\n",
        "\n",
        "img_data = list(zip(img_list,mask_list))\n",
        "\n",
        "data_len = len(img_data)"
      ],
      "metadata": {
        "id": "l4NA8sKWGvD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "\n",
        "learning_rate = 1e-5\n",
        "num_epochs = 300\n",
        "batch_size = 7\n",
        "num_batches = data_len//batch_size"
      ],
      "metadata": {
        "id": "wwj0uLy1wKZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting into 80-10-10\n",
        "\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(img_data, [round(0.8*data_len), round(0.1*data_len), data_len - round(0.8*data_len) - round(0.1*data_len)])"
      ],
      "metadata": {
        "id": "jxddbhITHQSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide Train Data Into List of Batches for Training Loop\n",
        "train_loader_x = []\n",
        "train_loader_y = []\n",
        "\n",
        "for idx in range(0, len(train_set), batch_size):\n",
        "  x_tup, y_tup = list(zip(*(list(train_set)[idx:idx + batch_size])))\n",
        "  train_loader_x.append(x_tup)\n",
        "  train_loader_y.append(y_tup)\n",
        "\n",
        "print(train_loader_x) #is a list of tuples, in which each tuple is batch_size length\n",
        "print(train_loader_y)"
      ],
      "metadata": {
        "id": "cncyGZGEKihM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582c7673-3232-45f7-f87e-46a652937314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('out/image/1_7.png', 'out/image/1_13.png', 'out/image/1_12.png', 'out/image/1_3.png', 'out/image/1_21.png', 'out/image/1_5.png', 'out/image/1_2.png'), ('out/image/1_18.png', 'out/image/1_17.png', 'out/image/1_20.png', 'out/image/1_6.png', 'out/image/1_19.png', 'out/image/1_4.png', 'out/image/1_23.png'), ('out/image/1_26.png', 'out/image/1_24.png', 'out/image/1_16.png', 'out/image/1_9.png', 'out/image/1_1.png', 'out/image/1_25.png', 'out/image/1_22.png')]\n",
            "[('out/mask/1_7.png', 'out/mask/1_13.png', 'out/mask/1_12.png', 'out/mask/1_3.png', 'out/mask/1_21.png', 'out/mask/1_5.png', 'out/mask/1_2.png'), ('out/mask/1_18.png', 'out/mask/1_17.png', 'out/mask/1_20.png', 'out/mask/1_6.png', 'out/mask/1_19.png', 'out/mask/1_4.png', 'out/mask/1_23.png'), ('out/mask/1_26.png', 'out/mask/1_24.png', 'out/mask/1_16.png', 'out/mask/1_9.png', 'out/mask/1_1.png', 'out/mask/1_25.png', 'out/mask/1_22.png')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Optimizer, Loss Function"
      ],
      "metadata": {
        "id": "_-Pr3kgbg1c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "double_u_net = DoubleUNet(torch.ones(batch_size, 3, 288, 384))\n",
        "# for parameter in double_u_net.parameters():\n",
        "#    print(f\"Parameter = {parameter}\")\n",
        "\n",
        "optimizer = optim.NAdam(double_u_net.parameters(), lr = 0.001)\n",
        "\n",
        "criterion  = nn.BCELoss()"
      ],
      "metadata": {
        "id": "R4oix4kTg13Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store Losses"
      ],
      "metadata": {
        "id": "qE_wF2tElmCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []"
      ],
      "metadata": {
        "id": "oT--tO7zlma1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "yVqd3LW5guZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epochs in tqdm(range(num_epochs)):\n",
        "  running_loss = 0\n",
        "  for idx in tqdm(range(num_batches)):\n",
        "    img_data = [img_to_tensor(read_img(ele)) for ele in train_loader_x[idx]]\n",
        "    img_data = torch.cat(img_data, dim = 0)\n",
        "    mask_data = [mask_to_tensor(read_mask(ele)).repeat(2, 1, 1) for ele in train_loader_y[idx]]\n",
        "    mask_data = torch.stack(mask_data, dim = 0)\n",
        "    #print(mask_data.shape)\n",
        "    mask_pred = double_u_net.forward(img_data.float())\n",
        "    #print(mask_pred.shape)\n",
        "    loss = criterion(mask_pred, mask_data)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    losses.append(running_loss)\n",
        "    #break\n",
        "  print(f\"For epoch {epochs}, BCE Loss is {running_loss}\")\n",
        "  #break"
      ],
      "metadata": {
        "id": "M2EdLcoWI9Hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5a82f6-bc51-4587-bea9-c6e02ab43d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/300 [00:00<?, ?it/s]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 33%|███▎      | 1/3 [01:24<02:48, 84.32s/it]\u001b[A\n",
            " 67%|██████▋   | 2/3 [02:47<01:23, 83.68s/it]\u001b[A\n",
            "100%|██████████| 3/3 [04:08<00:00, 82.78s/it]\n",
            "  0%|          | 1/300 [04:08<20:37:35, 248.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For epoch 0, BCE Loss is 2.0387314558029175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 33%|███▎      | 1/3 [01:20<02:41, 80.65s/it]\u001b[A\n",
            " 67%|██████▋   | 2/3 [02:40<01:20, 80.21s/it]\u001b[A\n",
            "100%|██████████| 3/3 [04:02<00:00, 80.81s/it]\n",
            "  1%|          | 2/300 [08:10<20:16:12, 244.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For epoch 1, BCE Loss is 1.8975027799606323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 33%|███▎      | 1/3 [01:21<02:42, 81.02s/it]\u001b[A\n",
            " 67%|██████▋   | 2/3 [02:42<01:21, 81.41s/it]\u001b[A\n",
            "100%|██████████| 3/3 [04:04<00:00, 81.53s/it]\n",
            "  1%|          | 3/300 [12:15<20:11:29, 244.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For epoch 2, BCE Loss is 1.7948530912399292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t_Eyn0L9u89y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}